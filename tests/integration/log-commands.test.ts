/**
 * Log Commands Tests
 *
 * These tests verify the awf logs subcommands:
 * - awf logs - view proxy logs
 * - awf logs stats - show aggregated statistics
 * - awf logs summary - generate summary report
 * - Log source discovery
 */

/// <reference path="../jest-custom-matchers.d.ts" />

import { describe, test, expect, beforeAll, afterAll, beforeEach, afterEach } from '@jest/globals';
import { createRunner, AwfRunner } from '../fixtures/awf-runner';
import { cleanup } from '../fixtures/cleanup';
import { createLogParser } from '../fixtures/log-parser';
import * as fs from 'fs';
import * as path from 'path';

describe('Log Commands', () => {
  let runner: AwfRunner;

  beforeAll(async () => {
    await cleanup(false);
    runner = createRunner();
  });

  afterAll(async () => {
    await cleanup(false);
  });

  test('should generate logs during firewall operation', async () => {
    const result = await runner.runWithSudo(
      'curl -f --max-time 10 https://api.github.com/zen',
      {
        allowDomains: ['github.com'],
        logLevel: 'debug',
        keepContainers: true,
        timeout: 60000,
      }
    );

    expect(result).toSucceed();

    // Check that logs were created
    if (result.workDir) {
      const squidLogPath = path.join(result.workDir, 'squid-logs', 'access.log');

      // Logs may not be immediately available due to buffering
      // Wait a moment for logs to be flushed
      await new Promise(resolve => setTimeout(resolve, 1000));

      if (fs.existsSync(squidLogPath)) {
        const logContent = fs.readFileSync(squidLogPath, 'utf-8');
        expect(logContent.length).toBeGreaterThan(0);
      }
    }

    // Cleanup after test
    await cleanup(false);
  }, 120000);

  test('should parse log entries correctly', async () => {
    const result = await runner.runWithSudo(
      'bash -c "curl -f https://api.github.com/zen && curl -f https://example.com 2>&1 || true"',
      {
        allowDomains: ['github.com'],
        logLevel: 'debug',
        keepContainers: true,
        timeout: 60000,
      }
    );

    // First curl should succeed, second should fail
    if (result.workDir) {
      const squidLogPath = path.join(result.workDir, 'squid-logs', 'access.log');

      await new Promise(resolve => setTimeout(resolve, 1000));

      if (fs.existsSync(squidLogPath)) {
        const logContent = fs.readFileSync(squidLogPath, 'utf-8');
        const parser = createLogParser();
        const entries = parser.parseSquidLog(logContent);

        // Should have at least one entry
        if (entries.length > 0) {
          // Verify entry structure
          const entry = entries[0];
          expect(entry).toHaveProperty('timestamp');
          expect(entry).toHaveProperty('host');
          expect(entry).toHaveProperty('statusCode');
          expect(entry).toHaveProperty('decision');
        }
      }
    }

    await cleanup(false);
  }, 120000);

  test('should distinguish allowed vs blocked requests in logs', async () => {
    const result = await runner.runWithSudo(
      'bash -c "curl -f --max-time 10 https://api.github.com/zen; curl -f --max-time 5 https://example.com 2>&1 || true"',
      {
        allowDomains: ['github.com'],
        logLevel: 'debug',
        keepContainers: true,
        timeout: 120000,
      }
    );

    if (result.workDir) {
      const squidLogPath = path.join(result.workDir, 'squid-logs', 'access.log');

      await new Promise(resolve => setTimeout(resolve, 1000));

      if (fs.existsSync(squidLogPath)) {
        const logContent = fs.readFileSync(squidLogPath, 'utf-8');
        const parser = createLogParser();
        const entries = parser.parseSquidLog(logContent);

        // Filter by decision
        const allowed = parser.filterByDecision(entries, 'allowed');
        const blocked = parser.filterByDecision(entries, 'blocked');

        // We should have at least one allowed (github.com) and one blocked (example.com)
        // Note: Log parsing depends on timing and buffering
        if (entries.length > 0) {
          expect(allowed.length + blocked.length).toBeGreaterThanOrEqual(1);
        }
      }
    }

    await cleanup(false);
  }, 180000);
});

describe('Log Parser Functionality', () => {
  test('should parse Squid log format correctly', () => {
    const parser = createLogParser();

    // Sample log line in firewall_detailed format
    const logLine = '1705500000.123 172.30.0.20:45678 api.github.com 140.82.121.6:443 HTTP/1.1 CONNECT 200 TCP_TUNNEL:HIER_DIRECT api.github.com:443 "curl/7.88.1"';

    const entries = parser.parseSquidLog(logLine);

    expect(entries).toHaveLength(1);
    expect(entries[0].host).toBe('api.github.com');
    expect(entries[0].statusCode).toBe(200);
    expect(entries[0].decision).toBe('TCP_TUNNEL');
  });

  test('should identify blocked entries correctly', () => {
    const parser = createLogParser();

    // Sample blocked log line
    const logLine = '1705500000.123 172.30.0.20:45678 example.com 0.0.0.0:443 HTTP/1.1 CONNECT 403 TCP_DENIED:HIER_NONE example.com:443 "curl/7.88.1"';

    const entries = parser.parseSquidLog(logLine);
    const blocked = parser.filterByDecision(entries, 'blocked');

    expect(blocked).toHaveLength(1);
    expect(blocked[0].host).toBe('example.com');
    expect(blocked[0].statusCode).toBe(403);
  });

  test('should get unique domains from log entries', () => {
    const parser = createLogParser();

    const logLines = `
1705500000.123 172.30.0.20:45678 api.github.com 140.82.121.6:443 HTTP/1.1 CONNECT 200 TCP_TUNNEL:HIER_DIRECT api.github.com:443 "curl/7.88.1"
1705500001.456 172.30.0.20:45679 raw.github.com 185.199.108.133:443 HTTP/1.1 CONNECT 200 TCP_TUNNEL:HIER_DIRECT raw.github.com:443 "curl/7.88.1"
1705500002.789 172.30.0.20:45680 api.github.com 140.82.121.6:443 HTTP/1.1 CONNECT 200 TCP_TUNNEL:HIER_DIRECT api.github.com:443 "curl/7.88.1"
`;

    const entries = parser.parseSquidLog(logLines);
    const domains = parser.getUniqueDomains(entries);

    expect(domains).toContain('api.github.com');
    expect(domains).toContain('raw.github.com');
    expect(domains).toHaveLength(2);
  });

  test('should filter entries by domain', () => {
    const parser = createLogParser();

    const logLines = `
1705500000.123 172.30.0.20:45678 api.github.com 140.82.121.6:443 HTTP/1.1 CONNECT 200 TCP_TUNNEL:HIER_DIRECT api.github.com:443 "curl/7.88.1"
1705500001.456 172.30.0.20:45679 example.com 93.184.216.34:443 HTTP/1.1 CONNECT 403 TCP_DENIED:HIER_NONE example.com:443 "curl/7.88.1"
`;

    const entries = parser.parseSquidLog(logLines);
    const githubEntries = parser.filterByDomain(entries, 'github.com');

    expect(githubEntries).toHaveLength(1);
    expect(githubEntries[0].host).toBe('api.github.com');
  });
});
